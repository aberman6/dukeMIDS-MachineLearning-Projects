{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Machine Learning Basics\n",
    "\n",
    "## *Anna Berman*\n",
    "Netid:  *aeb100*\n",
    "\n",
    "Github link: <https://github.com/aberman6/machine-learning-course/blob/master/Assignment_2.ipynb>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives:\n",
    "1. Be able to apply basic regression and classification supervised learning techniques to data and evaluate the performance of those methods\n",
    "2. Understand the bias-variance tradeoff and how adjusting model flexibility impacts model selection and the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceptual Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "**[5 points]**\n",
    "For each part (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.\n",
    "\n",
    "1. The sample size $n$ is extremely large, and the number of predictors $p$ is small.\n",
    "2. The number of predictors $p$ is extremely large, and the number of observations $n$ is small.\n",
    "3. The relationship between the predictors and response is highly non-linear.\n",
    "4. The variance of the error terms, i.e. $\\sigma^2 = Var(\\epsilon)$, is extremely high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. If the sample is extremely large and the number of predictors is small, a more flexible will likely preform **better** than an inflexible model. The more flexible model will more closely fit the data, and because n is large an p is small we are not very concerned with overfitting the training data.\n",
    "\n",
    "\n",
    "2. If the sample is small and the number of predictors is extremely large, a more flexible model will likely perform **worse** than an inflexible model. Although the flexible model will more closely fit the data, a flexible model fit on a small n and large p has a high chance of overfitting the training data.\n",
    "\n",
    "\n",
    "3. If the relationship between predictors and reponse is highly nonlinear, we expect the performance of a flexible statistical learning model to be **better** than an inflexible method. An inflexible method will force the highly nonlinear model into a more linear format. This will result in a high bias an a poor performance compared to a more flexible model that would more accurately explain the non-linear relationship.\n",
    "\n",
    "\n",
    "4. If the variance of the error tierms is extremely high, we expect the performance of a flexible statistical model to be **worse** than an inflexible method. If the variance of the error terms is extremely high, this may indicate that our model is fitting too tighly to the training data, or in other words, our model is overfitting the data. To reduce this variance and ultimately improve the performace of our model on our test data we need to decrease the flexibiity of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "**[5 points]** For each of the following, (i) explain if each scenario is a classification or regression problem, (ii) indicate whether we are most interested in inference or prediction for that problem, and (iii) provide the sample size $n$ and number of predictors $p$ indicated for each scenario.\n",
    "\n",
    "**(a)** We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary.\n",
    "\n",
    "**(b)** We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.\n",
    "\n",
    "**(c)** We are interesting in predicting the % change in the US dollar in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the dollar, the % change in the US market, the % change in the British market, and the % change in the German market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(ai)** Regression - outcome is CEO salary (continuous)\n",
    "\n",
    "**(aii)** Inference - What features, or predictors, affect CEO salary?\n",
    "\n",
    "**(aiii)** n = 500 (US firms), \n",
    "p = 3 {profit, number of employees, industry}\n",
    "\n",
    "\n",
    "**(bi)** Classification - outcome is success or failure (binary)\n",
    "\n",
    "**(bii)** Prediction - Will a new product be a success or failture?\n",
    "\n",
    "**(biii)** n = 20 (products), \n",
    "p = 13 {price, marketing budget, competition price, 10 other}\n",
    "\n",
    "\n",
    "**(ci)** Regression - outcome is % change in US dollar (continous)\n",
    "\n",
    "**(cii)** Prediciton - What will be the % change each week\n",
    "\n",
    "**(ciii)** n = 52 (stock market data each week in 2012), \n",
    "p = 3 {% change in the US market, % change in the British market, % change in the German market}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\n",
    "**[20 points] Classification I: Creating a classification algorithm**.\n",
    "\n",
    "**(a)** Build a working version of a binary kNN classifier using the skeleton code below.\n",
    "\n",
    "**(b)** Load the datasets to be evaluated here. Each includes training features ($\\mathbf{X}$), and test features ($\\mathbf{y}$) for both a low dimensional ($p = 2$ features/predictors) and a high dimensional ($p = 100$ features/predictors). For each of these datasets there are $n=100$ observations of each. They can be found in the `data` subfolder in the `assignments` folder on github. Each file is labeled similar to `A2_X_train_low.csv`, which lets you know whether the dataset is of features, $X$, targets, $y$; training or testing; and low or high dimensions.\n",
    "\n",
    "**(c)** Train your classifier on first the low dimensional dataset and then the high dimensional dataset with $k=5$. Evaluate the classification performance on the corresponding test data for each. Calculate the time it takes to make the predictions in each case and the overall accuracy of each set of test data predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a) Write your own kNN classifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Load data\n",
    "X_train_low = pd.read_csv('/Users/annaberman/annaberman/data/A2_X_train_low.csv', \n",
    "                          header = None)\n",
    "y_train_low = pd.read_csv('/Users/annaberman/annaberman/data/A2_y_train_low.csv', \n",
    "                          header = None)\n",
    "X_test_low = pd.read_csv('/Users/annaberman/annaberman/data/A2_X_test_low.csv', \n",
    "                         header = None)\n",
    "y_test_low = pd.read_csv('/Users/annaberman/annaberman/data/A2_y_test_low.csv', \n",
    "                         header = None)\n",
    "\n",
    "X_train_high = pd.read_csv('/Users/annaberman/annaberman/data/A2_X_train_high.csv', \n",
    "                           header = None)\n",
    "y_train_high = pd.read_csv('/Users/annaberman/annaberman/data/A2_y_train_high.csv', \n",
    "                           header = None)\n",
    "X_test_high = pd.read_csv('/Users/annaberman/annaberman/data/A2_X_test_high.csv', \n",
    "                          header = None)\n",
    "y_test_high = pd.read_csv('/Users/annaberman/annaberman/data/A2_y_test_high.csv', \n",
    "                          header = None)\n",
    "\n",
    "\n",
    "class Knn:\n",
    "# k-Nearest Neighbor class object for classification training and testing\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        # Save the training data to properties of this class\n",
    "        self.predictors = x\n",
    "        self.outcomes = y\n",
    "        pass\n",
    "    \n",
    "    def predict(self, x, k):\n",
    "        # Calculate distances between each predictor and new x\n",
    "        distances = all_distances(x, self.predictors)\n",
    "\n",
    "        # Find the k nearest neighbors for each point\n",
    "        k_nearest = find_k_nearest(distances, k)\n",
    "        \n",
    "        # Calculate predictions\n",
    "        y_hat = [] # Variable to store the estimated class label for \n",
    "        for i in distances.index:\n",
    "            # Loops through every new x, gets the neighbors class, \n",
    "            # finds mode (y_hat)\n",
    "            # For every new x get the indexes of the nearest neighbors\n",
    "            nearest_predictors = k_nearest.iloc[i,:]\n",
    "            # Match the indexes to the y values of nearest neighbors\n",
    "            nearest_outcomes = self.outcomes.iloc[nearest_predictors]\n",
    "            # Calculate the most likely outcome value of neighbors\n",
    "            majority = nearest_outcomes.mode().iloc[0,0]\n",
    "            # Append prediction to y_hat\n",
    "            y_hat.append(majority)\n",
    "            pass\n",
    "        \n",
    "        # Return the estimated targets\n",
    "        return y_hat\n",
    "\n",
    "def all_distances(x, predictors):\n",
    "    # DataFrame to store the distances between each predictor and new point\n",
    "    # Row indexs = New x index\n",
    "    # Column indexs = Predictor indexs\n",
    "    # Ex: distances[0,1] == distance between 1st new x and 2nd predictor\n",
    "    distances = pd.DataFrame(0, index = np.arange(len(x)), \n",
    "                             columns = np.arange(len(predictors))) \n",
    "        # Calculate the distance from each vector in x to the training data\n",
    "    for i in range(len(x)):\n",
    "        #print('Calculating distance: ', i)\n",
    "        for j in range(len(predictors)):\n",
    "            # Loops through every new x and every predictor points\n",
    "            # Attachs euclidean distance to distance df\n",
    "            #euc_dist = np.linalg.norm(x.iloc[i,:] - np.array(predictors.iloc[j,:]))\n",
    "           \n",
    "            # Slower, homemade version\n",
    "            euc_dist = np.dot(np.subtract(x.iloc[i,:],\n",
    "                              np.array(predictors.iloc[j,:])),\n",
    "                              np.subtract(x.iloc[i,:],\n",
    "                              np.array(predictors.iloc[j,:])))\n",
    "            \n",
    "            distances.iloc[i,j] = euc_dist\n",
    "            pass\n",
    "        pass\n",
    "    return distances\n",
    "\n",
    "def find_k_nearest(distances, k):\n",
    "    # Find the k nearest neighbors for each point\n",
    "    k_nearest = pd.DataFrame()\n",
    "    for i in distances.index:\n",
    "        # Loops through every new x finds the closest predictor points distances\n",
    "        # Get distances for single new point\n",
    "        new_x_dist = distances.iloc[i,:] \n",
    "        # Sort distances\n",
    "        new_x_dist = new_x_dist.sort_values() \n",
    "        # Get the indexes of nearest neighbors\n",
    "        k_indexs = pd.Series(new_x_dist.index[:k], index = np.arange(k))\n",
    "        # Append to df of nearest neighbor indexes\n",
    "        k_nearest = k_nearest.append(k_indexs, ignore_index = True)\n",
    "        pass\n",
    "    return k_nearest\n",
    "\n",
    "\n",
    "# Metric of overall classification accuracy\n",
    "#  (a more general function, sklearn.metrics.accuracy_score, is also available)\n",
    "def accuracy(y,y_hat):\n",
    "    nvalues = len(y)\n",
    "    accuracy = sum(y == y_hat) / nvalues\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of your kNN classifier on a low- and \n",
    "# a high-dimensional dataset and time the predictions of each\n",
    "\n",
    "# Run low dimensional model with k = 5\n",
    "start_time = time.time()\n",
    "model = Knn()\n",
    "model.fit(X_train_low, y_train_low)\n",
    "y_hat = model.predict(X_test_low, 5)\n",
    "end_time = time.time()\n",
    "total_time_low = end_time - start_time\n",
    "print('Total time low-dimensional:', round(total_time_low,2), \n",
    "      'secs')\n",
    "\n",
    "# Test accuracy of model\n",
    "y = np.array(y_test_high[0].tolist())\n",
    "y_hat = np.array(y_hat)\n",
    "accuracy_low = accuracy(y, y_hat)\n",
    "print('Accuracy low-dimensional:', accuracy_low)\n",
    "\n",
    "\n",
    "# Run low dimensional model with k = 5\n",
    "start_time = time.time()\n",
    "model = Knn()\n",
    "model.fit(X_train_high, y_train_high)\n",
    "y_hat = model.predict(X_test_high, 5)\n",
    "end_time = time.time()\n",
    "total_time_high = end_time - start_time\n",
    "print('Total high-dimensional:', round(total_time_high,2), \n",
    "      'secs')\n",
    "\n",
    "# Test accuracy of model\n",
    "y = np.array(y_test_high[0].tolist())\n",
    "y_hat = np.array(y_hat)\n",
    "accuracy_high = accuracy(y, y_hat)\n",
    "print('Accuracy high-dimensional:', accuracy_high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** Compare your implementation's accuracy and computation time to the scikit learn [KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) class. How do the results and speed compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Low dimensional comparison to sklearn\n",
    "start_time = time.time()\n",
    "model = KNeighborsClassifier(5)\n",
    "model.fit(X_train_low, np.array(y_train_low).ravel())\n",
    "y_hat = model.predict(X_test_low)\n",
    "end_time = time.time()\n",
    "total_time_low_sklearn = end_time - start_time\n",
    "\n",
    "print('Total time low-dimensional sklearn:', round(total_time_low_sklearn,2), 'secs')\n",
    "print('Sklearn is ', round(total_time_low/total_time_low_sklearn,2), \n",
    "      'times faster than our classifer for our low dimensional data.\\n',\n",
    "      'The speed of sklearn is much faster.\\n')\n",
    "\n",
    "y = np.array(y_test_low[0].tolist())\n",
    "y_hat = np.array(y_hat)\n",
    "accuracy_low_sklearn = accuracy_score(y, y_hat)\n",
    "print('Accuracy low-dimensional sklearn:', accuracy_low_sklearn)\n",
    "print('Sklearn is ', round(accuracy_low/accuracy_low_sklearn,3), \n",
    "      'times more accurate than our classifer for our low dimensional data.\\n',\n",
    "      'The accuracy is relatively the same.\\n')\n",
    "\n",
    "# High dimensional comparison to sklearn\n",
    "start_time = time.time()\n",
    "model = KNeighborsClassifier(5)\n",
    "model.fit(X_train_high, np.array(y_train_high).ravel())\n",
    "y_hat = model.predict(X_test_high)\n",
    "end_time = time.time()\n",
    "total_time_high_sklearn = end_time - start_time\n",
    "\n",
    "print('Total time high-dimensional sklearn:', round(total_time_high_sklearn,2), 'secs')\n",
    "print('Sklearn is ', round(total_time_high/total_time_high_sklearn,2), \n",
    "      'times faster than our classifer for our high dimensional data,\\n',\n",
    "      'The speed of sklearn is much faster.\\n')\n",
    "\n",
    "y = np.array(y_test_high[0].tolist())\n",
    "y_hat = np.array(y_hat)\n",
    "accuracy_high_sklearn = accuracy_score(y, y_hat)\n",
    "print('Accuracy high-dimensional sklearn:', round(accuracy_high_sklearn,2), 'secs')\n",
    "print('Sklearn is ', round(accuracy_high/accuracy_high_sklearn,3), \n",
    "      'times more accurate than our classifer for our high dimensional data.\\n',\n",
    "      'The accuracy is relatively the same.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to our calculations, the accuracy of both models are the same however the scikit classifer is much faster than our version. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)** Some supervised learning algorithms are more computationally intensive during training than testing. What are the drawbacks of the prediction process being slow?\n",
    "\n",
    "There are certainly contexts for analysis that require fast predictions (such as in financial trading and missile defense). If this was an analysis that needed to be computed many times a day, or even an ad-hoc analysis that required very quick results, the speed of our homemade KNN classifier may not be able to deliver the results needed in the desired time frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\n",
    "**[10 points] Classification II**. The table below provides a training dataset containing six observations ($n=6$), three predictors ($p=3$), and one qualitative response variable.\n",
    "\n",
    "*Table 1. Dataset with $n=6$ observations in $p=3$ dimensions with a categorical response, $y$*\n",
    "\n",
    "| Obs. | $x_1$ | $x_2$ | $x_3$ | $y$   |\n",
    "|------|-------|-------|-------|-------|\n",
    "| **1**| 0     | 3     | 0     | Red   |\n",
    "| **2**| 2     | 0     | 0     | Red   |\n",
    "| **3**| 0     | 1     | 3     | Red   |\n",
    "| **4**| 0     | 1     | 2     | Blue  |\n",
    "| **5**| -1    | 0     | 1     | Blue  |\n",
    "| **6**| 1     | 1     | 1     | Red   |\n",
    "\n",
    "We want to use this dataset to make a prediction for $y$ when $x_1=x_2=x_3=0$ using $K$-nearest neighbors. You are given some code below to get you started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Compute the Euclidean distance between each observation and the test point, $x_1=x_2=x_3=0$. Present your answer in a table similar in style to Table 1 with observations 1-6 as the row headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Observations\n",
    "X = np.array([[ 0, 3, 0],\n",
    "              [ 2, 0, 0],\n",
    "              [ 0, 1, 3],\n",
    "              [ 0, 1, 2],\n",
    "              [-1, 0, 1],\n",
    "              [ 1, 1, 1]])\n",
    "# Response\n",
    "y = np.array(['r','r','r','b','b','r'])\n",
    "\n",
    "# Test point x1 = x2 = x3\n",
    "new_x = np.array([0,0,0])\n",
    "\n",
    "# Euclidean distance\n",
    "#distances = pd.DataFrame()\n",
    "distances = []\n",
    "for obs in X:\n",
    "    euc_dist = np.linalg.norm(new_x - obs)\n",
    "    distances.append(euc_dist)\n",
    "    pass\n",
    "\n",
    "table = pd.DataFrame({'Euclidean distance from test point':distances, \n",
    "                     'Response': y}, index = np.arange(1,7))\n",
    "table = table.rename_axis('Obs.')\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** What is our prediction with $K=1$? Why?\n",
    "\n",
    "With $K=1$, our predictions is blue because the first closest point from our test point is observation 5 which is class b. See table below with observations and responses sorted by distances from test point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observations and responses sorted by distances from test point\n",
    "table.sort_values('Euclidean distance from test point')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** What is our prediction with $K=3$? Why?\n",
    "\n",
    "With $K = 3$ we can see that two of the three nearest neighbors are classified as red and one is classified as blue. Thus the probabilities for the class of our prediction is the $P(blue) = 1/3$ and $P(red) = 2/3$. With $K = 3$. Given these probabilities, our prediction is that our test point is red because $P(red) > P(blue)$. See table below with three nearest observations and responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observations and responses sorted by distances from test point\n",
    "# Three nearest neighbors\n",
    "table.sort_values('Euclidean distance from test point').iloc[:3, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** If the Bayes decision boundary (the optimal decision boundary) in this problem is highly nonlinear, then would we expect the *best* value of $K$ to be large or small? Why?\n",
    "\n",
    "In general, decreasing $K$ results in a more flexible model. If the Bayes decision boundary is highly nonlinear, we would therefore expect the *best* values of $K$ to be small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5\n",
    "**[20 points] Bias-variance tradeoff I: Understanding the tradeoff**. This exercise will illustrate the impact of the bias-variance tradeoff on classifier performance by looking at classifier decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Create a synthetic dataset (with both features and targets). Use the [`make_moons`](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html#sklearn.datasets.make_moons) module with the parameter `noise=0.35` to generate 1000 random samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import random\n",
    "\n",
    "random.seed(133)\n",
    "\n",
    "# Create synthetic dataset\n",
    "X, y = make_moons(1000, noise = .35)\n",
    "df = pd.DataFrame({'x1': X[:,0], \n",
    "                   'x2': X[:,1], \n",
    "                   'class': y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Scatterplot your random samples with each class in a different color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create color maps\n",
    "cmap_light = ListedColormap(['#CECEEE', '#fef65b'])\n",
    "cmap_bold = ListedColormap(['#876387', '#daa520'])\n",
    "\n",
    "# Scatterplot\n",
    "plt.scatter(X[:, 0], X[:,1], c = y, cmap = cmap_bold)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('Random Sample n=1000')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Create 3 different data subsets by selecting 100 of the 1000 data points at random three times. For each of these 100-sample datasets, fit three k-Nearest Neighbor classifiers with: $k = \\{1, 25, 50\\}$. This will result in 9 combinations (3 datasets, with 3 trained classifiers).\n",
    "\n",
    "**(d)** For each combination of dataset trained classifier, in a 3-by-3 grid, plot the decision boundary (similar in style to Figure 2.15 from *Introduction to Statistical Learning*). Each column should represent a different value of $k$ and each row should represent a different dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting decision region\n",
    "x_min, x_max = df['x1'].min() - 1, df['x1'].max() + 1\n",
    "y_min, y_max = df['x2'].min() - 1, df['x2'].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "# Creating subplots\n",
    "fig, axs = plt.subplots(3, 3, dpi = 300)\n",
    "fig.set_figwidth(14)\n",
    "fig.set_figheight(14)\n",
    "\n",
    "# Defining Ks\n",
    "ks = [1,25,50]\n",
    "for i in range(3):\n",
    "    # Generate random sample\n",
    "    sample_index = random.sample(list(df.index), 100)\n",
    "    sample = df.iloc[sample_index,:]\n",
    "    \n",
    "    for k_index, k in enumerate(ks):\n",
    "        # Fit KNN\n",
    "        knn = KNeighborsClassifier(n_neighbors = k)\n",
    "        knn.fit(sample[['x1', 'x2']], sample['class'])\n",
    "        \n",
    "        # Make predictions across entire decision region\n",
    "        Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        \n",
    "        # Plot decision regions and original data\n",
    "        axs[i,k_index].pcolormesh(xx, yy, Z, cmap=cmap_light, alpha = .5)\n",
    "        axs[i,k_index].scatter(sample['x1'], sample['x2'], \n",
    "                               c=sample['class'], cmap=cmap_bold)\n",
    "        axs[i,k_index].set_xlim(x_min, x_max)\n",
    "        axs[i,k_index].set_ylim(y_min, y_max)\n",
    "        axs[i,k_index].title.set_text('Sample ' + str(i+1) +': K =' + str(k))\n",
    "        \n",
    "        # Format axes\n",
    "        if i != 2: axs[i,k_index].set_xticks([], [])\n",
    "        else: axs[i,k_index].set_xlabel('x1')\n",
    "        if k_index != 0: axs[i,k_index].set_yticks([], [])\n",
    "        else: axs[i,k_index].set_ylabel('x2')\n",
    "            \n",
    "        pass\n",
    "    pass\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "fig.suptitle('Bias-Variance Tradeoff:\\nThree samples, K = {1,25,50}',\n",
    "            fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)** What do you notice about the difference between the rows and the columns. Which decision boundaries appear to best separate the two classes of data? Which decision boundaries vary the most as the data change?\n",
    "\n",
    "Looking at the decision boundary for $K = 1$, we see there is a great deal of variability between each sample. In fact, each point is perfectly classified. This model appears to be over fitting each sample. This decision boundary variability decreases with $K = 25$, and decreases even further with $K = 50$. \n",
    "\n",
    "Additionally, regardless of sample, as $K$ increases we see the decision boundary look more and more like straight line. In other words, the model is becoming less and less flexible as $K$ increases. As a result of this increasing rigidity, as $K$ increases we see an increase in misclassification. \n",
    "\n",
    "Given the options of $K = {1, 25, 50}$, the best option appears to be $K = 25$ because this yields a classification model with the best balance of consistency between samples and correct classification (or the best bias-variance tradeoff). \n",
    "\n",
    "\n",
    "**(f)** Explain the bias-variance tradeoff using the example of the plots you made in this exercise.\n",
    "\n",
    "The bias-variance tradeoff states that, as a general rule, as we use more flexible methods, the variance will increase and the bias will decrease. What we see in our plots illustrates this tradeoff. \n",
    "\n",
    "As $K$ decreases, or as the flexibility increases, we see that variability in the model classification increases while bias decreases. With $K = 1$ we see that the decision boundary varies quite a lot between each sample, but that each point is classified perfectly. \n",
    "\n",
    "On the other hand, as $K$ increases, or as flexibility decreases, we see that variability decreases while bias increases. With $K = 50$ we see a very consistent boundary between each sample. However, the decision boundary does not seem to do a good job explaining the actually pattern in the data and ultimately misclassifies a good deal of points. \n",
    "\n",
    "The best model fit is one that balances variance and bias. In this case, the best choice is $K = 25$ as it shows the best attempt to balance model variance and bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6\n",
    "**[20 points] Bias-variance trade-off II: Quantifying the tradeoff**. This exercise will explore the impact of the bias-variance tradeoff on classifier performance by looking at classifier decision boundaries.\n",
    "\n",
    "Here, the value of $k$ determines how flexible our model is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Using the function created earlier to generate random samples (using the `make_moons` function), create a new set of 1000 random samples, and call this dataset your test set and the previously created dataset your training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic training dataset\n",
    "#X, y = make_moons(1000, noise = .35)\n",
    "\n",
    "# Create synthetic test dataset\n",
    "X_test, y_test = make_moons(1000, noise = .35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Train a kNN classifier on your training set for $k = 1,2,...500$. Apply each of these trained classifiers to both your training dataset and your test dataset and plot the classification error (fraction of mislabeled datapoints)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(250)\n",
    "\n",
    "# Train Knn Classifiers \n",
    "scores = []\n",
    "for i in range(1,501):\n",
    "    # Fit a knn model with k = i\n",
    "    knn = KNeighborsClassifier(n_neighbors = i)\n",
    "    knn.fit(X, y)\n",
    "    \n",
    "    # Generate predictions\n",
    "    accuracy = knn.score(X_test, y_test)\n",
    "    scores.append(accuracy)\n",
    "    pass\n",
    "\n",
    "# Plot error rate    \n",
    "plt.plot(np.arange(1,501), np.subtract(1,scores))\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Classification Error')\n",
    "plt.title('Bias-Variance Tradeoff: Classification Error Over Increasing K')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** What trend do you see in the results?\n",
    "\n",
    "When $K$ is very small, the classification error is relatively large. As $K$ increases, there is an initial very stark reduction in classification error, however after $K \\approx 15$ the classification error begins to slowly increase again and eventually rises rapidly as $K$ continues to increase.\n",
    "\n",
    "**(d)** What values of $k$ represent high bias and which represent high variance?\n",
    "\n",
    "The smallest values of $K$ represents the highest variance. In the extreme at $K = 1$, if a prediction's class is determined by the class of it's single nearest neighbor, this model will be highly flexible and also highly variable to small changes in the data. On the flip side, the largest values of $K$ represent the highest bias. When $K = 500$, a prediction's class is determined by half of the entire dataset. This will force a very consistent, but nearly linear decision boundary as we saw in question 6. Forcing this shape on our decision boundary creates a large bias as $K$ continues to increase.!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)** What is the optimal value of $k$ and why?\n",
    "\n",
    "According to our misclassification rates, the K associated with the best preforming model is the kNN with $K = 15$. Although the exact number will shift with randomization, the optimal $K$ should generally fall around 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print K for the model with the lowest misclassification\n",
    "scores = np.array(scores)\n",
    "print(np.argmax(scores) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f)** In kNN classifiers, the value of k controls the flexibility of the model - what controls the flexibility of other models?\n",
    "\n",
    "In parametric models, the number of parameters often the determines the flexibility of the model. An increasing number of parameters is associated with increased flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7\n",
    "**[20 points] Linear regression and nonlinear transformations**. You're given a dataset below that is partitioned into a training and testing dataset. Your goal is to develop a regression algorithm from the training data that performs well on the test data.\n",
    "\n",
    "*Hint: Use the scikit learn [LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) module.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [3.19,9.26,9.38,8.77,7.91,3.79,3.18,7.61,2.36,6.26,6.62,1.53,6.25,7.93,7.07,\n",
    "           4.58,4.14,2.14,9.04,4.56,3.99,6.71,2.51,0.84,6.13,5.22,0.25,3.60,1.36,5.59,\n",
    "           4.81,1.14,0.36,2.31,1.37,5.86,4.23,9.48,2.26,0.77,4.33]\n",
    "y_train = [46.40,172.16,209.00,203.31,82.88,62.57,14.38,177.00,8.01,82.35,84.84,-5.59,\n",
    "           54.96,167.17,83.09,-21.63,94.64,63.97,106.57,38.99,88.26,66.99,-11.12,-0.44,\n",
    "           65.08,61.47,-0.61,23.85,10.55,83.69,54.35,51.82,-18.63,1.98,4.90,55.44,50.09,\n",
    "           155.66,45.57,18.12,30.58]\n",
    "\n",
    "x_test = [5.65,0.07,8.84,5.14,6.65,1.35,5.45,7.39,3.35]\n",
    "y_test = [98.52,16.09,198.45,75.90,85.11,47.64,14.76,141.03,-39.13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Create a scatter plot of your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data scatter plot\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Training Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Estimate a linear regression model ($y = a_0 + a_1 x$) for the training data and calculate both the $R^2$ value and mean square error for the fit of that model for the training data. Also provide the equation representing the estimated model (e.g. $y = a_0 + a_1 x$, but with the estimated coefficients inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Reshape data for LinearRegression\n",
    "x_train1 = np.array([x_train]).reshape(-1,1)\n",
    "y_train = np.array([y_train]).reshape(-1,1)\n",
    "\n",
    "x_test = np.array([x_test]).reshape(-1,1)\n",
    "y_test = np.array([y_test]).reshape(-1,1)\n",
    "\n",
    "# Fit linear model\n",
    "fit1 = LinearRegression().fit(x_train1, y_train)\n",
    "print('Linear regression estimation: y =', \n",
    "      round(fit1.intercept_[0],2), '+', \n",
    "      round(fit1.coef_[0][0],2), 'x')\n",
    "\n",
    "# R squared and MSE\n",
    "#r2_1 = r2_score(y_train, pred1)\n",
    "#mse1 = mean_squared_error(y_train, pred1)\n",
    "pred1 = fit1.predict(x_train1)\n",
    "SSReg = np.sum((y_train - pred1) * (y_train - pred1))\n",
    "SSE = np.sum((y_train - np.mean(y_train)) * (y_train - np.mean(y_train)))\n",
    "\n",
    "r2_1 = 1- (SSReg / SSE)\n",
    "mse_1 = SSReg / len(y_train)\n",
    "print('\\nLinear relationship - train data - R-squared:', round(r2_1,2))\n",
    "print('\\nLinear relationship - train data - MSE:', round(mse_1,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** A linear model does not mean that non-linear relationships cannot be explored. From looking at the scatter plot of the training data, choose a transformation of the predictor variable, $x$ that may make sense for these data. This will be a multiple regression model of the form $y = a_0 + a_1 x_1 + a_2 x_2 + \\ldots + a_n x_n$. Here $x_i$ could be any transformations of x - perhaps it's $\\frac{1}{x}$, $log(x)$, $sin(x)$, $x^k$ (where $k$ is any power of your choosing). Provide the estimated equation for this multiple regression model (e.g. if you chose your predictors to be $x_1 = x$ and $x_2 = log(x)$, your model would be of the form $y = a_0 + a_1 x + a_2 log(x)$. Also provide the $R^2$ and mean square error of the fit for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat quadratic x term\n",
    "x2 = x_train1*x_train1\n",
    "x_train2 = np.column_stack((x_train, x2))\n",
    "\n",
    "# Fit linear model\n",
    "fit2 = LinearRegression().fit(x_train2, y_train)\n",
    "print('Linear regression estimation: y =',\n",
    "      round(fit2.intercept_[0],2), '+', \n",
    "      round(fit2.coef_[0][0],2), 'x +',\n",
    "      round(fit2.coef_[0][1],2), 'x^2')\n",
    "\n",
    "# R squared and MSE\n",
    "#r2_2 = r2_score(y_train, pred2)\n",
    "#mse_2 = mean_squared_error(y_train, pred2)\n",
    "pred2 = fit2.predict(x_train2)\n",
    "SSReg = np.sum((y_train - pred2) * (y_train - pred1))\n",
    "SSE = np.sum((y_train - np.mean(y_train)) * (y_train - np.mean(y_train)))\n",
    "r2_2 = 1 - (SSReg / SSE)\n",
    "mse_2 = SSReg / len(y_train)\n",
    "print('\\nNonlinear relationship - train data - R-squared', round(r2_2,2))\n",
    "print('\\nNonlinear relationship - train data - MSE:', round(mse_2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** Using both of the models you created here in (b) and (c), plot the original data (as a scatter plot) and the two curves representing your models (each as a separate line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prediction interval\n",
    "interval = np.arange(min(x_train), max(x_train), step = .01)\n",
    "interval = np.array([interval]).reshape(-1,1)\n",
    "interval2 = interval*interval\n",
    "interval2 = np.column_stack((interval, interval2))\n",
    "\n",
    "# Generate predicts\n",
    "y_hat1 = fit1.predict(interval)\n",
    "y_hat2 = fit2.predict(interval2)\n",
    "\n",
    "# Plot predictions\n",
    "plt.scatter(x_train, y_train, label = 'Original data')\n",
    "plt.plot(interval, y_hat1, 'r', label = 'Linear relationship')\n",
    "plt.plot(interval, y_hat2, 'g', label = 'Nonlinear relationship')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression on Training Data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)** Using the models above, apply them to the test data and estimate the $R^2$ and mean square error of the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "# Linear relationship\n",
    "pred3 = fit1.predict(x_test)\n",
    "r2_3 = r2_score(y_test, pred3)\n",
    "mse_3 = mean_squared_error(y_test, pred3)\n",
    "print('Linear relationship - test data - R-squared:', round(r2_3,2))\n",
    "print('Linear relationship - test data - MSE:',round(mse_3,2))\n",
    "\n",
    "# Nonlinear relationship\n",
    "x_test2 = x_test*x_test\n",
    "x_test2 = np.column_stack((x_test, x_test2))\n",
    "pred4 = fit2.predict(x_test2)\n",
    "r2_4 = r2_score(y_test, pred4)\n",
    "mse_4 = mean_squared_error(y_test, pred4)\n",
    "print('\\nNonlinear relationship - test data - R-squared:', round(r2_4,2))\n",
    "print('Nonlinear relationship - test data - MSE:', round(mse_4,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f)** Which models perform better on the training data, and which on the test data? Why?\n",
    "\n",
    "For both the training and the test dataset, our model with the nonlinear relationship performed better. From here on out I refer to these models as 'linear' and 'nonlinear' in respect to the relationships they represent, however both models are examples of linear regression.\n",
    "\n",
    "The performance of the 'nonlinear' model was only slightly better than the performance of the 'linear' model in our training dataset. However, the performance of the 'nonlinear' model using our test dataset was much stronger than that of the 'linear' model. It appears that a purely linear relationship includes too much bias in our model. Our 'linear' model imposing a pattern to the data that does not resemble the true underlying pattern and thus the performance of our 'linear' model deteriorates when applied to the test dataset. Our 'nonlinear' model on the other hand is more flexible and more closely explains the true pattern of the data. Therefore our 'nonlinear' model's performance on our test data is only slightly worse than that on the training data.\n",
    "\n",
    "**(g)** Imagine that the test data were significantly different from the training dataset. How might this affect the predictive capability of your model? Why?\n",
    "\n",
    "A model is only as good as the data it's trained on. If the training data is not representative of the test data, any model's predictive performance - no matter the model's sophistication - will significantly deteriorate when applied to data that is significantly different from its training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
